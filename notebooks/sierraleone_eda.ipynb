{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafc5ecc",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a26722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sierra Leone Solar Data Analysis\n",
    "# EDA for Sierra Leone solar dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6584",
   "metadata": {},
   "source": [
    "## Loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "print(\"=== LOADING SIERRA LEONE DATA ===\")\n",
    "\n",
    "df = pd.read_csv('../data/sierraleone-bumbuna.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22907481",
   "metadata": {},
   "source": [
    "## Summary Statistics & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"An overview of the dataset: \")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics for all numeric columns\n",
    "print(\"The description of the numeric columns:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview of categorical columns\n",
    "print(\"The description of the categorical columns:\")\n",
    "print(df.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The description of all columns:\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Analysis using also a percent to use it for later on Z-score\n",
    "print(\"\\n MISSING VALUES REPORT \")\n",
    "missing_data = df.isna().sum()\n",
    "missing_percent = (df.isna().sum() / len(df)) * 100\n",
    "\n",
    "missing_report = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "\n",
    "print(missing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f59089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "# Cardinality (uniqueness) for categoricals\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "cardinality = {c: df[c].nunique() for c in cat_cols}\n",
    "print(\"Cardinality (categoricals):\", cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with more than 5% missing values\n",
    "print(\"COLUMNS WITH >5% MISSING VALUES\")\n",
    "high_missing = missing_report[missing_report['Missing_Percent'] > 5]\n",
    "\n",
    "if not high_missing.empty:\n",
    "    print(\"Missing values:\")\n",
    "    print(high_missing)\n",
    "else:\n",
    "    print(\"No columns have more than 5% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760315a",
   "metadata": {},
   "source": [
    "# Outlier detection and Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9588eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key columns for outlier detection\n",
    "key_columns = ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'WS', 'WSgust']\n",
    "print(\"=== ANALYZING KEY COLUMNS FOR OUTLIERS ===\")\n",
    "print(\"Key columns:\", key_columns)\n",
    "\n",
    "# Check which of these columns exist in our data\n",
    "available_columns = [col for col in key_columns if col in df.columns]\n",
    "print(\"Available columns in dataset:\", available_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Z-scores and flag outliers (|Z| > 3)\n",
    "print(\"\\nOUTLIER DETECTION USING Z-SCORES\")\n",
    "\n",
    "# Create a copy of the dataframe for cleaning\n",
    "df_clean = df.copy()\n",
    "key_columns = ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'WS', 'WSgust']\n",
    "available_columns = [col for col in key_columns if col in df.columns]\n",
    "for column in available_columns:\n",
    "    # Calculate Z-scores\n",
    "    z_scores = np.abs(stats.zscore(df_clean[column].dropna()))\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = z_scores > 3\n",
    "    outlier_count = outliers.sum()\n",
    "    outlier_percent = (outlier_count / len(df_clean[column].dropna())) * 100\n",
    "    \n",
    "    print(f\"{column}: {outlier_count} outliers ({outlier_percent:.2f}%)\")\n",
    "    \n",
    "    # Flag outliers in the dataframe\n",
    "    df_clean[f'{column}_outlier'] = False\n",
    "    df_clean.loc[df_clean[column].notna(), f'{column}_outlier'] = outliers\n",
    "\n",
    "# Total rows with any outlier\n",
    "any_outlier = df_clean[[f'{col}_outlier' for col in available_columns]].any(axis=1)\n",
    "print(f\"\\nTotal rows with at least one outlier: {any_outlier.sum()} ({any_outlier.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop or impute missing values in key columns\n",
    "print(\"\\n MISSING VALUE IMPUTE\")\n",
    "\n",
    "for column in available_columns:\n",
    "    missing_before = df_clean[column].isna().sum()\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        # Impute with median (you can change to mean if preferred)\n",
    "        median_value = df_clean[column].median()\n",
    "        df_clean[column].fillna(median_value, inplace=True)\n",
    "        \n",
    "        missing_after = df_clean[column].isna().sum()\n",
    "        print(f\"{column}: Imputed {missing_before} missing values with median {median_value:.2f}\")\n",
    "\n",
    "print(\"\\nMissing values after treatment:\")\n",
    "print(df_clean[available_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b754d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export Cleaned Data\n",
    "print(\"=== EXPORTING CLEANED DATA ===\")\n",
    "\n",
    "df_clean.to_csv('../data/sierraleone_clean.csv', index=False)\n",
    "print(\"Cleaned data saved to: ../data/sierraleone_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a621c9",
   "metadata": {},
   "source": [
    "# Time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e279631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time Series Analysis\n",
    "print(\"=== TIME SERIES ANALYSIS ===\")\n",
    "\n",
    "# Convert timestamp\n",
    "df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'])\n",
    "df_clean['Hour'] = df_clean['Timestamp'].dt.hour\n",
    "df_clean['Month'] = df_clean['Timestamp'].dt.month\n",
    "\n",
    "# Plot time series for key variables\n",
    "variables_to_plot = ['GHI', 'DNI', 'DHI', 'Tamb']\n",
    "\n",
    "for var in variables_to_plot:\n",
    "    if var in df_clean.columns:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        # Plot first 500 points for clarity\n",
    "        plt.plot(df_clean['Timestamp'].iloc[:500], df_clean[var].iloc[:500])\n",
    "        plt.title(f'{var} Over Time (First 500 points)')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel(var)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede26af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Daily Patterns\n",
    "print(\"=== DAILY PATTERNS ===\")\n",
    "\n",
    "# Calculate hourly averages\n",
    "hourly_avg = df_clean.groupby('Hour')[variables_to_plot].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for var in variables_to_plot:\n",
    "    if var in hourly_avg.columns:\n",
    "        plt.plot(hourly_avg.index, hourly_avg[var], marker='o', label=var)\n",
    "\n",
    "plt.title('Average Daily Patterns by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monthly Patterns\n",
    "print(\"=== MONTHLY PATTERNS ===\")\n",
    "\n",
    "monthly_avg = df_clean.groupby('Month')[variables_to_plot].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for var in variables_to_plot:\n",
    "    if var in monthly_avg.columns:\n",
    "        plt.plot(monthly_avg.index, monthly_avg[var], marker='s', label=var)\n",
    "\n",
    "plt.title('Average Monthly Patterns')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbf8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect potential anomalies\n",
    "print(\"=== ANOMALY DETECTION ===\")\n",
    "\n",
    "# Find days with unusual patterns (e.g., very low GHI during daytime)\n",
    "daytime_hours = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "daytime_data = df_clean[df_clean['Hour'].isin(daytime_hours)]\n",
    "\n",
    "# Flag days with very low solar radiation during daytime\n",
    "low_ghi_days = daytime_data[daytime_data['GHI'] < 10]  # Adjust threshold as needed\n",
    "\n",
    "if not low_ghi_days.empty:\n",
    "    print(f\"Found {len(low_ghi_days)} records with very low GHI during daytime\")\n",
    "    print(\"These could be cloudy days, rainy days, or data issues\")\n",
    "    \n",
    "    # Show the dates with lowest GHI\n",
    "    low_ghi_summary = low_ghi_days.groupby(low_ghi_days['Timestamp'].dt.date).size()\n",
    "    print(\"\\nDates with most low-GHI records:\")\n",
    "    print(low_ghi_summary.sort_values(ascending=False).head(10))\n",
    "else:\n",
    "    print(\"No significant anomalies detected in daytime GHI values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c780bf",
   "metadata": {},
   "source": [
    "# Cleaning impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning Impact Analysis\n",
    "print(\"=== CLEANING IMPACT ===\")\n",
    "\n",
    "# Create cleaning flag\n",
    "df_clean['had_outlier'] = False\n",
    "outlier_cols = [f'{col}_outlier' for col in available_columns if f'{col}_outlier' in df_clean.columns]\n",
    "if outlier_cols:\n",
    "    df_clean['had_outlier'] = df_clean[outlier_cols].any(axis=1)\n",
    "\n",
    "# Compare sensor readings\n",
    "sensor_cols = ['ModA', 'ModB']\n",
    "available_sensors = [col for col in sensor_cols if col in df_clean.columns]\n",
    "\n",
    "if available_sensors and 'had_outlier' in df_clean.columns:\n",
    "    sensor_comparison = df_clean.groupby('had_outlier')[available_sensors].mean()\n",
    "    print(\"Average sensor readings by outlier status:\")\n",
    "    print(sensor_comparison)\n",
    "    \n",
    "    # Plot comparison\n",
    "    sensor_comparison.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title('Sierra Leone - Sensor Readings: Clean vs Outlier Data')\n",
    "    plt.ylabel('Average Value')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dfb5e",
   "metadata": {},
   "source": [
    "# Correlation & Relationship analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d37a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\" CORRELATION ANALYSIS \")\n",
    "\n",
    "# Define variables for correlation analysis included Tamb,RH,WS,WSgust and WD as they are used in scatter plots\n",
    "correlation_vars = ['GHI', 'DNI', 'DHI', 'TModA', 'TModB', 'Tamb', 'RH', 'WS', 'WSgust', 'WD']\n",
    "\n",
    "# Check which variables exist in our dataset\n",
    "available_corr_vars = [var for var in correlation_vars if var in df_clean.columns]\n",
    "print(f\"Variables available for correlation: {available_corr_vars}\")\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "corr_data = df_clean[available_corr_vars].select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"Data shape for correlation: {corr_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "print(\"\\nCORRELATION HEATMAP \")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = corr_data.corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "\n",
    "plt.title('Correlation Heatmap of Solar and Environmental Variables', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter Plot Relationships\n",
    "print(\"SCATTER PLOT RELATIONSHIPS\")\n",
    "\n",
    "# Wind vs GHI\n",
    "wind_vars = ['WS', 'WSgust', 'WD']\n",
    "available_wind = [var for var in wind_vars if var in df_clean.columns]\n",
    "\n",
    "if available_wind and 'GHI' in df_clean.columns:\n",
    "    fig, axes = plt.subplots(1, len(available_wind), figsize=(15, 5))\n",
    "    if len(available_wind) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, wind_var in enumerate(available_wind):\n",
    "        axes[i].scatter(df_clean[wind_var], df_clean['GHI'], alpha=0.5)\n",
    "        axes[i].set_xlabel(wind_var)\n",
    "        axes[i].set_ylabel('GHI')\n",
    "        axes[i].set_title(f'GHI vs {wind_var}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57645b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RH Relationships\n",
    "print(\"HUMIDITY RELATIONSHIPS\")\n",
    "\n",
    "if 'RH' in df_clean.columns:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # RH vs Tamb\n",
    "    if 'Tamb' in df_clean.columns:\n",
    "        ax1.scatter(df_clean['RH'], df_clean['Tamb'], alpha=0.5)\n",
    "        ax1.set_xlabel('RH')\n",
    "        ax1.set_ylabel('Tamb')\n",
    "        ax1.set_title('Temperature vs Humidity')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # RH vs GHI\n",
    "    if 'GHI' in df_clean.columns:\n",
    "        ax2.scatter(df_clean['RH'], df_clean['GHI'], alpha=0.5)\n",
    "        ax2.set_xlabel('RH')\n",
    "        ax2.set_ylabel('GHI')\n",
    "        ax2.set_title('Solar Radiation vs Humidity')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37081e",
   "metadata": {},
   "source": [
    "# Wind and Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wind Distribution Analysis\n",
    "print(\"=== WIND DISTRIBUTION ===\")\n",
    "\n",
    "if 'WS' in df_clean.columns and 'WD' in df_clean.columns:\n",
    "    # Simple wind direction histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_clean['WD'].dropna(), bins=36, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Wind Direction (degrees)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Wind Direction Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b91456",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variable Distributions\n",
    "print(\"=== VARIABLE DISTRIBUTIONS ===\")\n",
    "\n",
    "# Histograms for GHI and WS\n",
    "dist_vars = ['GHI', 'WS']\n",
    "available_dist = [var for var in dist_vars if var in df_clean.columns]\n",
    "\n",
    "for var in available_dist:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_clean[var].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc84b9",
   "metadata": {},
   "source": [
    "# Temperature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temperature-Humidity Analysis\n",
    "print(\"TEMPERATURE-HUMIDITY ANALYSIS\")\n",
    "\n",
    "if 'Tamb' in df_clean.columns and 'RH' in df_clean.columns:\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_clean['RH'], df_clean['Tamb'], alpha=0.5)\n",
    "    plt.xlabel('Relative Humidity (%)')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.title('Temperature vs Humidity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    corr = df_clean['RH'].corr(df_clean['Tamb'])\n",
    "    plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle=\"round\", facecolor=\"white\"))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e4736",
   "metadata": {},
   "source": [
    "# Bubble chart analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c695ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bubble Chart\n",
    "print(\"=== BUBBLE CHART ===\")\n",
    "\n",
    "if 'GHI' in df_clean.columns and 'Tamb' in df_clean.columns:\n",
    "    # Use RH for bubble size if available, otherwise BP\n",
    "    if 'RH' in df_clean.columns:\n",
    "        bubble_var = 'RH'\n",
    "        bubble_label = 'Relative Humidity'\n",
    "    elif 'BP' in df_clean.columns:\n",
    "        bubble_var = 'BP'\n",
    "        bubble_label = 'Barometric Pressure'\n",
    "    else:\n",
    "        bubble_var = None\n",
    "    \n",
    "    if bubble_var:\n",
    "        # Sample data for better visualization\n",
    "        sample_data = df_clean.iloc[::10]  # Every 10th point\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(sample_data['Tamb'], sample_data['GHI'],\n",
    "                             s=sample_data[bubble_var]/2,  # Scale bubble size\n",
    "                             alpha=0.6, \n",
    "                             c=sample_data[bubble_var],\n",
    "                             cmap='viridis')\n",
    "        \n",
    "        plt.xlabel('Temperature (°C)')\n",
    "        plt.ylabel('GHI (W/m²)')\n",
    "        plt.title(f'GHI vs Temperature\\n(Bubble Size = {bubble_label})')\n",
    "        plt.colorbar(scatter, label=bubble_label)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb862b4a",
   "metadata": {},
   "source": [
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary for Sierraleone EDA\n",
    "print(\" Sierraleone EDA COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "print(\"\\n ANALYSIS COMPLETED:\")\n",
    "completed_analyses = [\n",
    "    \"Data Loading & Initial Exploration\",\n",
    "    \"Missing Values & Outlier Detection\", \n",
    "    \"Data Cleaning & Export\",\n",
    "    \"Time Series Analysis\",\n",
    "    \"Cleaning Impact Assessment\",\n",
    "    \"Correlation & Relationship Analysis\",\n",
    "    \"Wind & Distribution Analysis\",\n",
    "    \"Temperature-Humidity Analysis\",\n",
    "    \"Bubble Chart Visualization\"\n",
    "]\n",
    "\n",
    "for analysis in completed_analyses:\n",
    "    print(f\"  {analysis}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14520ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export notebook state for reporting\n",
    "print(\"\\n=== EXPORTING ANALYSIS METRICS ===\")\n",
    "\n",
    "# Create summary statistics for reporting\n",
    "analysis_metrics = {\n",
    "    'total_records': len(df_clean),\n",
    "    'analysis_period_days': (df_clean['Timestamp'].max() - df_clean['Timestamp'].min()).days,\n",
    "    'data_quality_score': (len(df_clean) - df_clean.isna().sum().sum()) / len(df_clean) * 100,\n",
    "    'avg_ghi': df_clean['GHI'].mean() if 'GHI' in df_clean.columns else None,\n",
    "    'avg_temperature': df_clean['Tamb'].mean() if 'Tamb' in df_clean.columns else None,\n",
    "    'avg_humidity': df_clean['RH'].mean() if 'RH' in df_clean.columns else None,\n",
    "}\n",
    "\n",
    "print(\"Sierra Leone Analysis Metrics for Reporting:\")\n",
    "for metric, value in analysis_metrics.items():\n",
    "    if value is not None:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\n✅ Sierra Leone Exploratory Data Analysis COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
